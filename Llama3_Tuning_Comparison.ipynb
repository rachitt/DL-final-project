{"cells":[{"cell_type":"markdown","metadata":{"id":"markdown-cell-1"},"source":["## Sarcasm Detection Tuning:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6bODF4ByUZP","outputId":"d16633d3-6b9c-4b07-b487-07e446ac7f92","executionInfo":{"status":"ok","timestamp":1765556638864,"user_tz":300,"elapsed":2548,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","[Errno 2] No such file or directory: 'project_path'\n","/content\n"]}],"source":["# mount google drive and setup Kaggle\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","\n","project_path = \"/content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/\"\n","\n","\n","%cd project_path\n","\n","import os\n","# Download kaggle.json from https://www.kaggle.com/settings/account\n","# place kaggle.json in the below directory\n","os.environ['KAGGLE_CONFIG_DIR'] = project_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpnEZ2sMypGP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df332572-78cc-4a5a-99b3-df00ca345fd3","executionInfo":{"status":"ok","timestamp":1765556638867,"user_tz":300,"elapsed":2,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Success: The file 'Sarcasm_Headlines_Dataset_v2.json' was found at the specified path.\n"]}],"source":["# download data from Kaggle\n","import os\n","\n","file_path = project_path+\"Sarcasm_Headlines_Dataset_v2.json\"\n","\n","# Check if the file exists\n","if os.path.isfile(file_path):\n","  print(f\"âœ… Success: The file '{os.path.basename(file_path)}' was found at the specified path.\")\n","\n","else:\n","  print(f\"âŒ Error: The file '{os.path.basename(file_path)}' was NOT found.\")\n","\n","  # Download the dataset\n","  !kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection\n","\n","\n","  # Unzip the downloaded file\n","  !unzip news-headlines-dataset-for-sarcasm-detection.zip\n","\n","  !mkdir -p \"$project_path\"\n","\n","  !mv Sarcasm_Headlines_Dataset_v2.json \"$project_path\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"code-cell-1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a7d2232-7696-4639-95b4-428027707aef","executionInfo":{"status":"ok","timestamp":1765556651127,"user_tz":300,"elapsed":12259,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import json\n","import re\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","\n","from transformers import (\n","    BertTokenizer,\n","    BertForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding,\n",")\n","\n","from nltk.corpus import wordnet\n","import nltk\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n","\n","nltk.download('wordnet')\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n"]},{"cell_type":"markdown","metadata":{"id":"markdown-cell-2"},"source":["### Data Preparation (Loading, Cleaning, Augmentation, Tokenization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"code-cell-2"},"outputs":[],"source":["# --- Data Loading and Preprocessing ---\n","#path = \"Sarcasm_Headlines_Dataset_v2.json\"\n","path = file_path\n","data = []\n","\n","with open(path, \"r\") as f:\n","    for line in f:\n","        try:\n","            data.append(json.loads(line))\n","        except json.JSONDecodeError as e:\n","            print(f\"Skipping malformed JSON line: {line.strip()}. Error: {e}\")\n","df = pd.DataFrame(data)\n","df = df[[\"headline\", \"is_sarcastic\"]]\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # remove special chars\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","df[\"clean_headline\"] = df[\"headline\"].apply(clean_text)\n","\n","def synonym_replacement(sentence, n=1):\n","    words = sentence.split()\n","    new_words = words.copy()\n","    random_word_list = list(set([w for w in words if len(w) > 3]))\n","\n","    if len(random_word_list) == 0:\n","        return sentence\n","\n","    for _ in range(n):\n","        # Choose a random word from the list\n","        word_to_replace = random.choice(random_word_list)\n","        synonyms = wordnet.synsets(word_to_replace)\n","        if not synonyms:\n","            continue\n","\n","        # Choose a random synonym for better diversity\n","        synonym_words = random.choice(synonyms).lemma_names()\n","        synonym = synonym_words[0].replace(\"_\", \" \")\n","\n","        # Find the index of the word to replace in the original words list\n","        # This handles multiple occurrences, replacing only the first one found.\n","        try:\n","            idx = new_words.index(word_to_replace)\n","            new_words[idx] = synonym\n","        except ValueError:\n","            continue # Should not happen if word_to_replace came from words\n","\n","    return \" \".join(new_words)\n","\n","# Augmentation (only sarcastic data is augmented, as in the original notebook)\n","augmented_rows = []\n","for idx, row in df.iterrows():\n","    if row[\"is_sarcastic\"] == 1:\n","        aug = synonym_replacement(row[\"clean_headline\"])\n","        augmented_rows.append({\"clean_headline\": aug, \"is_sarcastic\": 1})\n","\n","aug_df = pd.DataFrame(augmented_rows)\n","df_augmented = pd.concat([df[[\"clean_headline\", \"is_sarcastic\"]], aug_df])\n","df_augmented.reset_index(drop=True, inplace=True)\n","\n","# Data Splitting\n","train_df, temp_df = train_test_split(df_augmented, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1705,"referenced_widgets":["0fcc1d4694094a2b80f650d94bcef81a","0ab97416c3474dcda58f2d73f017f543","69234508f0554d389b1ba5846fbcb6b7","b70ab5fec6b14ee08ca582bc38190287","af972cb2e5de4c4ea9a491e9aa1fd6ab","16201be0686c48b0ab4e58b8cb01f2b0","447c62bbdca64d13b2f46537512012a8","65d8e244f4e4462e81683755bddcfb3d","4e7f2babef314c1f905870774f26a465","ce0b1a4e70564166a1977202999a33e8","c291b5780e224914854852a821118653","8219972a427f414db3a525e62df1113e","2b390e127d7143159b53713501c5d7c5","a1b6abc372f548228cd49da0f11968e0","c52c0f96f6ac449fb36694249b670463","5807a4dbfe6b45a4bf32f80e6370249e","22badf11ca9a48d9a704b30904aadf70","344e3ad65efc4efeb97712d04333ba46","26b1d6aab7fc44bb8b15e8bdd24ada21","02198143acbd4bfcaa0d6d6854493aa7","97ca767794e5495ea84966e9619010a4","3c8a86f84fc44f89bb30a09784c93c2c","37c17d9b5a9b4e10940f27c4c006390c","3c086f84300c4ea8a61bbda587c5f6dc","daab1fb81a3d4cdea39513dc3997b589","4755ceea85a04104826ae300c0c9871b","ceaf3862458e4264856d19693980af43","6f81933b0ddb4b469643dda26ac9f933","6db7c0d6c2204ff4b83c25d80e2dd878","8d97181065a14176bdc9cb28c12395a0","416075c8b2744124b7c9d5694dbaf940","7f35280da2ff4f79a17b8add3d4e89db","7084e88775cf4bb285e759e3189aa790","ee7b2b159d034b82970d0abbdbecaa38","d82da7c880a94220820a0c627f2e77db","e082fc1edb12439492278477951ede08","2511276eb190488cab7078b084e127ee","3461eeab4fa543f9a5e6e7ce49822bfa","0ce7ab908a834b50aaa055a1fbe487cf","e1547be1105f46a9920dadfc2fd369a6","b492b6c5b1db4c69801b1ac14cd9bda8","3e3b34b508e14b308bdfe53f83c2ba9f","ab5685148609490d8e81375d6b43f9c3","56754486a89b4ebc99202562915e5456","b3b89cf83fe14599b04cac1bd4c6ca71","f64c2cc631944e69afb9e2121391018e","073a88a8c63f4fafa7ea671adea8cd67","e99237f8284d4ad4a353355be5aedd7c","45b2b87da21f49f8a6fcaedd95787c65","8df54fe0cd974870a7d81906d916c389","28d4189f389442b685f68b57e68947d3","b469bb7d656c46948e7402d8e294aaa2","a6695d5ec9814f5a9f469fdd9de06ff4","766e4454a9bd4d269371a6161dd99f03","d5322ca0905b40a69be650921b85da99","e0bd70c1c96a41f0aa23a7ecd7106e61","3712046fcdbd4efda16897b1f11e13f6","1fa2e7ac8c5346088b7f0e379dadc878","edd5278563eb466095d2573cee3a4f9d","cc6de16f13a94d6888bae95057b5074c","8814ecdbfdea419e981bf5b09929c0d3","906c2f5d951f40eea886ad577b63115b","fd59820a83f84fdea610f8db1f0ec6d3","8a8e23e7654f44f8acfcc1a38241df97","aaab130ba18b4f66b31bb5cfb145d6f3","525019f0e04d4c47bdf3d9eb17394bba"]},"id":"LdMRkh_aaoM7","outputId":"79ecee9f-af73-4d5a-c598-9bb46385797d","executionInfo":{"status":"ok","timestamp":1765556732334,"user_tz":300,"elapsed":74234,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing Unsloth dependencies for Colab environment...\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Requirement already satisfied: xformers==0.0.33.post1 in /usr/local/lib/python3.12/dist-packages (0.0.33.post1)\n","Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n","Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.22.2)\n","Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n","Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (25.1.1)\n","Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.12/dist-packages (2025.12.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n","Requirement already satisfied: datasets==4.3.0 in /usr/local/lib/python3.12/dist-packages (4.3.0)\n","Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (3.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.0.2)\n","Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (22.0.0)\n","Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.32.4)\n","Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.28.1)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2025.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (6.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (1.2.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (3.13.2)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (4.12.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.3.0) (0.16.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.3.0) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.3.0) (2.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.3.0) (1.17.0)\n","Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.12.5)\n","Requirement already satisfied: transformers==4.56.2 in /usr/local/lib/python3.12/dist-packages (4.56.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2025.11.12)\n","Requirement already satisfied: trl==0.22.2 in /usr/local/lib/python3.12/dist-packages (0.22.2)\n","ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3082461747.py:29: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"output_type":"stream","name":"stdout","text":["ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"]},{"output_type":"stream","name":"stderr","text":["Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n","Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Loading unsloth/Meta-Llama-3.1-8B...\n","==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.56.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fcc1d4694094a2b80f650d94bcef81a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8219972a427f414db3a525e62df1113e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c17d9b5a9b4e10940f27c4c006390c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee7b2b159d034b82970d0abbdbecaa38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b89cf83fe14599b04cac1bd4c6ca71"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Applying formatting to the training dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/33802 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0bd70c1c96a41f0aa23a7ecd7106e61"}},"metadata":{}}],"source":["# --- Setup for Unsloth Llama 3.1 8B Fine-Tuning ---\n","# Install unsloth and dependencies. Using %%capture to keep the notebook clean.\n","# NOTE: This cell will take some time to run the first time due to installations.\n","\n","# Use os.environ check for cleaner install in Colab, similar to the starter notebook\n","import os, re\n","from datetime import datetime\n","\n","# Check for Colab environment to determine installation path\n","if \"COLAB_GPU\" in os.environ:\n","    # Colab-specific installation for faster performance\n","    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n","    # The xformers version is often tightly coupled to the PyTorch version\n","    xformers = \"xformers==0.0.33.post1\" if v==\"2.9\" else \"xformers==0.0.32.post2\" if v==\"2.8\" else \"xformers==0.0.29.post3\"\n","\n","    # We use pip directly for the core installs, which can be verbose, so we put it outside a function.\n","    print(\"Installing Unsloth dependencies for Colab environment...\")\n","    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n","    !pip install --no-deps unsloth\n","else:\n","    # General installation for other environments\n","    print(\"Installing Unsloth dependencies...\")\n","    !pip install unsloth\n","\n","!pip install transformers==4.56.2\n","!pip install --no-deps trl==0.22.2\n","\n","from unsloth import FastLanguageModel\n","import torch\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","# Configuration\n","max_seq_length = 512 # Max context length for the model\n","dtype = None          # Auto-detect best data type (e.g., bfloat16)\n","load_in_4bit = True   # Use 4-bit quantization for efficiency\n","\n","# Load the base model and tokenizer\n","print(\"\\nLoading unsloth/Meta-Llama-3.1-8B...\")\n","model_llama, tokenizer_llama = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")\n","\n","\n","# The instructional prompt template for training (same as starter notebook)\n","training_prompt = \"\"\"You are an expert linguistic analysis agent trained to detect sarcasm. Your task is to analyze the provided text and determine if it is sarcastic or not.\n","\n","Input Text:\n","{}.\n","\n","Output 'True' if the text is sarcastic, otherwise 'False'.\n","\n","Output:\n","{}\n","\"\"\"\n","\n","# Must add an End Of Sequence (EOS) token\n","EOS_TOKEN = tokenizer_llama.eos_token\n","\n","# Function to format the dataset to the prompt template\n","def formatting_prompts_func(examples):\n","    # Use 'clean_headline' column from the augmented dataset\n","    headlines = examples[\"clean_headline\"]\n","    outputs = examples[\"is_sarcastic\"] #\n","\n","    # Convert numerical labels to \"True\"/\"False\" string for Llama instruction-tuning\n","    outputs = [(\"True\" if x == 1 else \"False\") for x in outputs]\n","    texts = []\n","\n","    for headline, output in zip(headlines, outputs):\n","        text = training_prompt.format(headline, output) + EOS_TOKEN\n","        texts.append(text)\n","\n","    return { \"text\" : texts }\n","\n","# Apply the formatting function to the training dataset from BERT_Tuning_Comparison_V2.ipynb\n","# 'ds' was defined in cell 4.\n","#df = pd.DataFrame(augmented_rows)\n","ds = Dataset.from_pandas(df_augmented)\n","\n","shuffled_dataset = ds.shuffle(seed=42)\n","\n","# First Split: Separate Train (80%) from the Rest (20%)\n","train_testvalid = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n","\n","# Second Split: Split the 'Rest' (20%) equally into Validation and Test\n","# Splitting the 20% in half gives you 10% Validation and 10% Test\n","test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n","\n","train_dataset = train_testvalid['train'] #.select(range(5000))\n","validation_dataset = test_valid['train'] #.select(range(1000))\n","test_dataset = test_valid['test'] #.select(range(1000))\n","\n","print(\"Applying formatting to the training dataset...\")\n","formatted_train_dataset_llama = train_dataset.map(formatting_prompts_func, batched=True)\n"]},{"cell_type":"markdown","metadata":{"id":"markdown-cell-5"},"source":["### Final Comparison of Test Set Metrics"]},{"cell_type":"code","source":["#Define the Inference and Evaluation Function ---\n","\n","# The prompt template must match the one used for training EXACTLY up to the 'Output:' line.\n","EVAL_PROMPT_TEMPLATE = \"\"\"You are an expert linguistic analysis agent trained to detect sarcasm. Your task is to analyze the provided text and determine if it is sarcastic or not.\n","\n","Input Text:\n","{}.\n","\n","Output 'True' if the text is sarcastic, otherwise 'False'.\n","\n","Output:\n","\"\"\"\n","\n","def evaluate_llama_model(model, tokenizer, test_dataset, prompt_template):\n","    \"\"\"\n","    Manually evaluates the Llama model on a test dataset by generating text.\n","    It compares the generated output ('True' or 'False') with the ground truth label.\n","    \"\"\"\n","    model.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Convert 'is_sarcastic' (0 or 1) to 'False' or 'True' string labels\n","    true_labels_str = [(\"True\" if x == 1 else \"False\") for x in test_dataset['is_sarcastic']]\n","\n","    # We will store the extracted predictions here\n","    predictions_str = []\n","\n","    print(f\"Starting inference on {len(test_dataset)} samples...\")\n","\n","    for i in range(len(test_dataset)):\n","        headline = test_dataset[i]['clean_headline']\n","\n","        # Format the prompt for inference\n","        prompt = prompt_template.format(headline)\n","\n","        # Tokenize the input prompt\n","        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n","\n","        # Generate a response. We expect a short output ('True' or 'False').\n","        # max_new_tokens=5 is a safe limit for 'True' or 'False'\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                input_ids=input_ids,\n","                max_new_tokens=5,\n","                use_cache=True,\n","                do_sample=False, # Use deterministic decoding\n","                pad_token_id=tokenizer.eos_token_id # Important for batching, though we do not batch here\n","            )\n","\n","        # Decode the output tokens and slice to get only the new generation\n","        generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()\n","\n","        # Simple extraction of 'True' or 'False' from the generated text\n","        # In a real-world scenario, you might need more robust parsing.\n","        predicted_label = 'False'\n","        if 'True' in generated_text:\n","             predicted_label = 'True'\n","        elif 'False' in generated_text:\n","             predicted_label = 'False'\n","        # Default to 'False' or handle as an error if neither is found\n","\n","        predictions_str.append(predicted_label)\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"Processed {i + 1}/{len(test_dataset)} samples.\")\n","\n","    # Convert string labels back to numerical (0 or 1) for metric calculation\n","    # True = 1, False = 0\n","    true_labels_int = [1 if x == 'True' else 0 for x in true_labels_str]\n","    predictions_int = [1 if x == 'True' else 0 for x in predictions_str]\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(true_labels_int, predictions_int)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        true_labels_int,\n","        predictions_int,\n","        average='binary',\n","        pos_label=1 # Calculate for the 'sarcastic' class\n","    )\n","\n","    # Placeholder for loss since SFTTrainer doesn't calculate it easily on eval\n","    eval_loss = None\n","\n","    return {\n","        \"eval_accuracy\": accuracy,\n","        \"eval_precision\": precision,\n","        \"eval_recall\": recall,\n","        \"eval_f1\": f1,\n","        \"eval_loss\": eval_loss\n","    }\n","\n","test_dataset = test_valid['test']"],"metadata":{"id":"27c9JDx6zYgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","from unsloth import FastLanguageModel\n","from trl import SFTTrainer\n","import torch\n","from datetime import datetime\n","import pandas as pd\n","import copy\n","\n","# Define a single function to encapsulate the entire fine-tuning process\n","def run_llama_experiment(\n","    experiment_name,\n","    train_dataset,\n","    test_dataset,\n","    base_model_name,\n","    lora_r,\n","    lora_alpha,\n","    num_train_epochs,\n","    learning_rate,\n","    lora_target_modules=None,\n","    use_rslora=True\n","):\n","    print(f\"\\n======== Starting Experiment: {experiment_name} ========\")\n","\n","    # Load Model and Tokenizer (This may take a moment)\n","    print(\"Loading model for experiment...\")\n","    # reload the model fresh for each experiment to ensure isolation.\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = base_model_name,\n","        max_seq_length = 512, # Consistent max seq length\n","        dtype = None,\n","        load_in_4bit = True,\n","    )\n","\n","    # Use default target modules if none are specified\n","    if lora_target_modules is None:\n","        lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                               \"gate_proj\", \"up_proj\", \"down_proj\"]\n","\n","    # Setup LoRA adapters\n","    model = FastLanguageModel.get_peft_model(\n","        model,\n","        r = lora_r,\n","        target_modules = lora_target_modules,\n","        lora_alpha = lora_alpha,\n","        lora_dropout = 0, # Keep dropout consistent for now\n","        bias = \"none\",\n","        use_gradient_checkpointing = \"unsloth\",\n","        random_state = 42,\n","        use_rslora=use_rslora,\n","    )\n","    print(f\"LoRA Config: r={lora_r}, alpha={lora_alpha}, epochs={num_train_epochs}, lr={learning_rate}\")\n","\n","    # Define Training Arguments\n","    training_args = TrainingArguments(\n","        output_dir = f\"./{experiment_name.replace(' ', '_').lower()}_\"+datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),\n","        num_train_epochs = num_train_epochs,\n","        per_device_train_batch_size = 64,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 10,\n","        learning_rate = learning_rate,\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        bf16 = torch.cuda.is_bf16_supported(),\n","        logging_steps = 50,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 42,\n","        report_to = \"none\",\n","    )\n","\n","    # Initialize and Run Trainer\n","    trainer = SFTTrainer(\n","        model = model,\n","        tokenizer = tokenizer,\n","        train_dataset = train_dataset,\n","        dataset_text_field = \"text\",\n","        max_seq_length = 512,\n","        dataset_num_proc = 2,\n","        packing = False,\n","        args = training_args,\n","    )\n","\n","    # Start training\n","    trainer.train()\n","\n","    output_dir = f\"{project_path}{experiment_name.replace(' ', '_').lower()}_\"+datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n","\n","    print(f\"\\nSaving LoRA weights to base model in {output_dir}...\")\n","\n","    # Create the directory if it doesn't exist\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Save the model and tokenizer\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","    print(f\"Model checkpoint and tokenizer saved to: {output_dir}\")\n","\n","    print(\"\\nStarting Test Set Evaluation...\")\n","    # Make a copy of the test dataset just in case the eval modifies it\n","    test_metrics = evaluate_llama_model(model, tokenizer, test_dataset, EVAL_PROMPT_TEMPLATE)\n","\n","    # Collate and return results\n","    results = {\n","        'Experiment': experiment_name,\n","        'R_Rank': lora_r,\n","        'Alpha': lora_alpha,\n","        'Epochs': num_train_epochs,\n","        'LR': learning_rate,\n","        'eval_loss': test_metrics['eval_loss'],\n","        'eval_accuracy': test_metrics['eval_accuracy'],\n","        'eval_precision': test_metrics['eval_precision'],\n","        'eval_recall': test_metrics['eval_recall'],\n","        'eval_f1': test_metrics['eval_f1']\n","    }\n","\n","    return results\n"],"metadata":{"id":"zxan30VVeGCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comparison_results_detailed = []"],"metadata":{"id":"ow40qetRj_4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# base line:\n","experiment_low_lr = run_llama_experiment(\n","    experiment_name = 'Llama 3.1 8B base line (LR=2e-4 1 Epoch)',\n","    train_dataset = formatted_train_dataset_llama,\n","    test_dataset = test_dataset,\n","    base_model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    lora_r = 16,\n","    lora_alpha = 32,\n","    num_train_epochs = 1,\n","    learning_rate = 2e-4,\n",")\n","comparison_results_detailed.append(experiment_low_lr)"],"metadata":{"id":"TriuKyZ_RqTp","colab":{"base_uri":"https://localhost:8080/","height":1348,"referenced_widgets":["638d44abb345436bb67568d088c460ac","16450f1a163841958dd621f5cd0c9054","62c8b0452132405bbc070f502fc88a22","18cea43d35024f189741f19cbb880da2","f3d4effcd0674462a83dc49ef36296e1","c5745381ab7a42eb884b2a162e4f0f60","a29f5df9e3274ec58d152b0de2cf555a","61930b55964e4efd8a79d72ce3028c60","a0ae3d85951f4a03b006adbf52825ecd","87299a8c35a443fd99c013494fcfcc35","6bdb0095e1c3498da1c7185560f2fc1b"]},"executionInfo":{"status":"ok","timestamp":1765558543240,"user_tz":300,"elapsed":1810836,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}},"outputId":"80754dc7-4047-480b-87d8-cd88d28e8c7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Starting Experiment: Llama 3.1 8B base line (LR=2e-4 1 Epoch) ========\n","Loading model for experiment...\n","==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.56.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.12.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]},{"output_type":"stream","name":"stdout","text":["LoRA Config: r=16, alpha=32, epochs=1, lr=0.0002\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/33802 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638d44abb345436bb67568d088c460ac"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 33,802 | Num Epochs = 1 | Total steps = 133\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n"," \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='133' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [133/133 11:33, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.986400</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.747900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Saving and Merging LoRA weights to base model in /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_base_line_(lr=2e-4_1_epoch)_2025-12-12_16-38-04...\n","Model checkpoint and tokenizer saved to: /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_base_line_(lr=2e-4_1_epoch)_2025-12-12_16-38-04\n","\n","Starting Test Set Evaluation...\n","Starting inference on 4226 samples...\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/4226 samples.\n","Processed 200/4226 samples.\n","Processed 300/4226 samples.\n","Processed 400/4226 samples.\n","Processed 500/4226 samples.\n","Processed 600/4226 samples.\n","Processed 700/4226 samples.\n","Processed 800/4226 samples.\n","Processed 900/4226 samples.\n","Processed 1000/4226 samples.\n","Processed 1100/4226 samples.\n","Processed 1200/4226 samples.\n","Processed 1300/4226 samples.\n","Processed 1400/4226 samples.\n","Processed 1500/4226 samples.\n","Processed 1600/4226 samples.\n","Processed 1700/4226 samples.\n","Processed 1800/4226 samples.\n","Processed 1900/4226 samples.\n","Processed 2000/4226 samples.\n","Processed 2100/4226 samples.\n","Processed 2200/4226 samples.\n","Processed 2300/4226 samples.\n","Processed 2400/4226 samples.\n","Processed 2500/4226 samples.\n","Processed 2600/4226 samples.\n","Processed 2700/4226 samples.\n","Processed 2800/4226 samples.\n","Processed 2900/4226 samples.\n","Processed 3000/4226 samples.\n","Processed 3100/4226 samples.\n","Processed 3200/4226 samples.\n","Processed 3300/4226 samples.\n","Processed 3400/4226 samples.\n","Processed 3500/4226 samples.\n","Processed 3600/4226 samples.\n","Processed 3700/4226 samples.\n","Processed 3800/4226 samples.\n","Processed 3900/4226 samples.\n","Processed 4000/4226 samples.\n","Processed 4100/4226 samples.\n","Processed 4200/4226 samples.\n"]}]},{"cell_type":"code","source":["# New Experiment 1: Lower LoRA Rank (Test for Efficiency)\n","experiment_low_r = run_llama_experiment(\n","    experiment_name = 'Llama 3.1 8B (Low Rank=8)',\n","    train_dataset = formatted_train_dataset_llama,\n","    test_dataset = test_dataset,\n","    base_model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    lora_r = 8,\n","    lora_alpha = 16,\n","    num_train_epochs = 1,\n","    learning_rate = 2e-4,\n",")\n","comparison_results_detailed.append(experiment_low_r)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1296,"referenced_widgets":["da8b6f630e3c4723ac2adddb52d75df0","ecae2efff8b344c9a5740b7d1c206539","66624584d4924bff883718baa68a254d","3b043ba68e544d518c2a9215f893da55","a1ed36318657498b851300d045e14955","4e16a799cf404f0caf44c67d6647c36a","cd014f82044a44e88788dd8103d70114","3aaf99aeb6ba46e0a6aba052273c4813","19d0359a97c24367b0f0fc19749e8cba","dea45e738b41479fb1c3a44e868c35b5","c431a0afdfc449daabb438fd5d1c0bf7"]},"id":"KwClc5BdeWyG","outputId":"e23b20ad-0f7a-489c-9270-ad02517ff87c","executionInfo":{"status":"ok","timestamp":1765560341500,"user_tz":300,"elapsed":1798259,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Starting Experiment: Llama 3.1 8B (Low Rank=8) ========\n","Loading model for experiment...\n","==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.56.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","LoRA Config: r=8, alpha=16, epochs=1, lr=0.0002\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/33802 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da8b6f630e3c4723ac2adddb52d75df0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 33,802 | Num Epochs = 1 | Total steps = 133\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n"," \"-____-\"     Trainable parameters = 20,971,520 of 8,051,232,768 (0.26% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='133' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [133/133 11:28, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>1.057800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.761700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Saving and Merging LoRA weights to base model in /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(low_rank=8)_2025-12-12_17-07-50...\n","Model checkpoint and tokenizer saved to: /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(low_rank=8)_2025-12-12_17-07-50\n","\n","Starting Test Set Evaluation...\n","Starting inference on 4226 samples...\n","Processed 100/4226 samples.\n","Processed 200/4226 samples.\n","Processed 300/4226 samples.\n","Processed 400/4226 samples.\n","Processed 500/4226 samples.\n","Processed 600/4226 samples.\n","Processed 700/4226 samples.\n","Processed 800/4226 samples.\n","Processed 900/4226 samples.\n","Processed 1000/4226 samples.\n","Processed 1100/4226 samples.\n","Processed 1200/4226 samples.\n","Processed 1300/4226 samples.\n","Processed 1400/4226 samples.\n","Processed 1500/4226 samples.\n","Processed 1600/4226 samples.\n","Processed 1700/4226 samples.\n","Processed 1800/4226 samples.\n","Processed 1900/4226 samples.\n","Processed 2000/4226 samples.\n","Processed 2100/4226 samples.\n","Processed 2200/4226 samples.\n","Processed 2300/4226 samples.\n","Processed 2400/4226 samples.\n","Processed 2500/4226 samples.\n","Processed 2600/4226 samples.\n","Processed 2700/4226 samples.\n","Processed 2800/4226 samples.\n","Processed 2900/4226 samples.\n","Processed 3000/4226 samples.\n","Processed 3100/4226 samples.\n","Processed 3200/4226 samples.\n","Processed 3300/4226 samples.\n","Processed 3400/4226 samples.\n","Processed 3500/4226 samples.\n","Processed 3600/4226 samples.\n","Processed 3700/4226 samples.\n","Processed 3800/4226 samples.\n","Processed 3900/4226 samples.\n","Processed 4000/4226 samples.\n","Processed 4100/4226 samples.\n","Processed 4200/4226 samples.\n"]}]},{"cell_type":"code","source":["# New Experiment 2: Higher Epochs (Test for Overfitting/Convergence)\n","experiment_high_epochs = run_llama_experiment(\n","    experiment_name = 'Llama 3.1 8B (2 Epochs)',\n","    train_dataset = formatted_train_dataset_llama,\n","    test_dataset = test_dataset,\n","    base_model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    lora_r = 16,\n","    lora_alpha = 32,\n","    num_train_epochs = 2,\n","    learning_rate = 2e-4,\n",")\n","comparison_results_detailed.append(experiment_high_epochs)"],"metadata":{"id":"tcOaKvnIeZeX","colab":{"base_uri":"https://localhost:8080/","height":1390,"referenced_widgets":["57900396aad44394b325dac3e140bd8e","db7ceefd371049709fcc719140c4c713","98b29cc31ab9416aaa74c466efef9918","ccd44101c1914bb0a4f18e32448a60f6","14941fe804a748fbada2021ff6068fb0","7d8a7c532b9143128d21a0dcee4de073","a8e634f43282449ab54ec7bdb5947eba","5138df876c2b473696008b72df478bc8","270a5373b0b44103a33a1bbdd158f085","84a592ce933e470a8dbe0083c627d404","bd7a8cccb3b94df8803b2ff867d8e059"]},"executionInfo":{"status":"ok","timestamp":1765562853846,"user_tz":300,"elapsed":2512342,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}},"outputId":"f47c395e-1366-4716-ed91-65a0a57466ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Starting Experiment: Llama 3.1 8B (2 Epochs) ========\n","Loading model for experiment...\n","==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.56.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","LoRA Config: r=16, alpha=32, epochs=2, lr=0.0002\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/33802 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57900396aad44394b325dac3e140bd8e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 33,802 | Num Epochs = 2 | Total steps = 266\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n"," \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='266' max='266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [266/266 23:12, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.986900</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.745600</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.688500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.611100</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.592100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Saving and Merging LoRA weights to base model in /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(2_epochs)_2025-12-12_17-49-31...\n","Model checkpoint and tokenizer saved to: /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(2_epochs)_2025-12-12_17-49-31\n","\n","Starting Test Set Evaluation...\n","Starting inference on 4226 samples...\n","Processed 100/4226 samples.\n","Processed 200/4226 samples.\n","Processed 300/4226 samples.\n","Processed 400/4226 samples.\n","Processed 500/4226 samples.\n","Processed 600/4226 samples.\n","Processed 700/4226 samples.\n","Processed 800/4226 samples.\n","Processed 900/4226 samples.\n","Processed 1000/4226 samples.\n","Processed 1100/4226 samples.\n","Processed 1200/4226 samples.\n","Processed 1300/4226 samples.\n","Processed 1400/4226 samples.\n","Processed 1500/4226 samples.\n","Processed 1600/4226 samples.\n","Processed 1700/4226 samples.\n","Processed 1800/4226 samples.\n","Processed 1900/4226 samples.\n","Processed 2000/4226 samples.\n","Processed 2100/4226 samples.\n","Processed 2200/4226 samples.\n","Processed 2300/4226 samples.\n","Processed 2400/4226 samples.\n","Processed 2500/4226 samples.\n","Processed 2600/4226 samples.\n","Processed 2700/4226 samples.\n","Processed 2800/4226 samples.\n","Processed 2900/4226 samples.\n","Processed 3000/4226 samples.\n","Processed 3100/4226 samples.\n","Processed 3200/4226 samples.\n","Processed 3300/4226 samples.\n","Processed 3400/4226 samples.\n","Processed 3500/4226 samples.\n","Processed 3600/4226 samples.\n","Processed 3700/4226 samples.\n","Processed 3800/4226 samples.\n","Processed 3900/4226 samples.\n","Processed 4000/4226 samples.\n","Processed 4100/4226 samples.\n","Processed 4200/4226 samples.\n"]}]},{"cell_type":"code","source":["# New Experiment 3: Lower Learning Rate (Test for Generalization)\n","experiment_low_lr = run_llama_experiment(\n","    experiment_name = 'Llama 3.1 8B (LR=2e-5)',\n","    train_dataset = formatted_train_dataset_llama,\n","    test_dataset = test_dataset,\n","    base_model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    lora_r = 16,\n","    lora_alpha = 32,\n","    num_train_epochs = 1,\n","    learning_rate = 2e-5,\n",")\n","comparison_results_detailed.append(experiment_low_lr)"],"metadata":{"id":"KiqMKkw9ebNG","colab":{"base_uri":"https://localhost:8080/","height":1296,"referenced_widgets":["f3b7dc79349d4632a23a7c1b04437f66","af2998a2333a430abd42236a527bd21f","9d32a2de7b464bf6bba267c047bcd667","0c0558a744d949e58c6da6b79e515920","3a8fc6c603064f35a625ebcd921b91f9","46430e98516e4cda97b6378ef654a543","585d0e338e35407689e0fa5aacbc7171","df54201603234e65895a03fc55478bb8","88e25116699c411fbde04b012d7e5a88","2a4804a5e0044776a013463733f0fe9e","b0ea38b9ae4f4baa8653507231bf000e"]},"executionInfo":{"status":"ok","timestamp":1765564682025,"user_tz":300,"elapsed":1828176,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}},"outputId":"18c2b098-b75f-4a85-f4b8-15b681070e01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Starting Experiment: Llama 3.1 8B (LR=2e-5) ========\n","Loading model for experiment...\n","==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.56.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","LoRA Config: r=16, alpha=32, epochs=1, lr=2e-05\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/33802 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b7dc79349d4632a23a7c1b04437f66"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 33,802 | Num Epochs = 1 | Total steps = 133\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n"," \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='133' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [133/133 11:33, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>1.280600</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.787600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Saving and Merging LoRA weights to base model in /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(lr=2e-5)_2025-12-12_18-19-46...\n","Model checkpoint and tokenizer saved to: /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(lr=2e-5)_2025-12-12_18-19-46\n","\n","Starting Test Set Evaluation...\n","Starting inference on 4226 samples...\n","Processed 100/4226 samples.\n","Processed 200/4226 samples.\n","Processed 300/4226 samples.\n","Processed 400/4226 samples.\n","Processed 500/4226 samples.\n","Processed 600/4226 samples.\n","Processed 700/4226 samples.\n","Processed 800/4226 samples.\n","Processed 900/4226 samples.\n","Processed 1000/4226 samples.\n","Processed 1100/4226 samples.\n","Processed 1200/4226 samples.\n","Processed 1300/4226 samples.\n","Processed 1400/4226 samples.\n","Processed 1500/4226 samples.\n","Processed 1600/4226 samples.\n","Processed 1700/4226 samples.\n","Processed 1800/4226 samples.\n","Processed 1900/4226 samples.\n","Processed 2000/4226 samples.\n","Processed 2100/4226 samples.\n","Processed 2200/4226 samples.\n","Processed 2300/4226 samples.\n","Processed 2400/4226 samples.\n","Processed 2500/4226 samples.\n","Processed 2600/4226 samples.\n","Processed 2700/4226 samples.\n","Processed 2800/4226 samples.\n","Processed 2900/4226 samples.\n","Processed 3000/4226 samples.\n","Processed 3100/4226 samples.\n","Processed 3200/4226 samples.\n","Processed 3300/4226 samples.\n","Processed 3400/4226 samples.\n","Processed 3500/4226 samples.\n","Processed 3600/4226 samples.\n","Processed 3700/4226 samples.\n","Processed 3800/4226 samples.\n","Processed 3900/4226 samples.\n","Processed 4000/4226 samples.\n","Processed 4100/4226 samples.\n","Processed 4200/4226 samples.\n"]}]},{"cell_type":"code","source":["# New Experiment 4:\n","experiment_low_lr = run_llama_experiment(\n","    experiment_name = 'Llama 3.1 8B (LR=2e-4 3 Epochs)',\n","    train_dataset = formatted_train_dataset_llama,\n","    test_dataset = test_dataset,\n","    base_model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    lora_r = 32,\n","    lora_alpha = 64,\n","    num_train_epochs = 3,\n","    learning_rate = 2e-4,\n",")\n","comparison_results_detailed.append(experiment_low_lr)"],"metadata":{"id":"EI4JIbDG8XoM","colab":{"base_uri":"https://localhost:8080/","height":1453,"referenced_widgets":["ed1b5a96cb3a44e3990f569a70eeb5bb","0fdd087aeb61435d9572f80ac5d3356b","1778d7dbf638447ba642fad1df9830fe","e9722a2803af468e99ac0f26782f6767","f7e21e7e3d3944baaa585e203f29ad1f","fc2d6ec07a7f4b86b34069ed0dc16e71","793fa850d1de4525a6708cf7bc9586b0","1622de9d47d747eab0198e7137e2891a","aea0467b05774311ba17f518454e8873","3a5f5a1a744e4fcb9efc8865d2ef24c4","6f31d41554fb4fc08dd88a322302400b"]},"executionInfo":{"status":"ok","timestamp":1765567874727,"user_tz":300,"elapsed":3192700,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}},"outputId":"2482f38f-b8be-4547-a636-e539bc56e087"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Starting Experiment: Llama 3.1 8B (LR=2e-4 3 Epochs) ========\n","Loading model for experiment...\n","==((====))==  Unsloth 2025.12.5: Fast Llama patching. Transformers: 4.56.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","LoRA Config: r=32, alpha=64, epochs=3, lr=0.0002\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/33802 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1b5a96cb3a44e3990f569a70eeb5bb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 33,802 | Num Epochs = 3 | Total steps = 399\n","O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n"," \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='399' max='399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [399/399 34:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.943000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.739600</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.623400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.480600</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.453400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.317200</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.259900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Saving and Merging LoRA weights to base model in /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(lr=2e-4_3_epochs)_2025-12-12_19-13-21...\n","Model checkpoint and tokenizer saved to: /content/gdrive/MyDrive/CS-GY 6953 DL/Final Project/llama_3.1_8b_(lr=2e-4_3_epochs)_2025-12-12_19-13-21\n","\n","Starting Test Set Evaluation...\n","Starting inference on 4226 samples...\n","Processed 100/4226 samples.\n","Processed 200/4226 samples.\n","Processed 300/4226 samples.\n","Processed 400/4226 samples.\n","Processed 500/4226 samples.\n","Processed 600/4226 samples.\n","Processed 700/4226 samples.\n","Processed 800/4226 samples.\n","Processed 900/4226 samples.\n","Processed 1000/4226 samples.\n","Processed 1100/4226 samples.\n","Processed 1200/4226 samples.\n","Processed 1300/4226 samples.\n","Processed 1400/4226 samples.\n","Processed 1500/4226 samples.\n","Processed 1600/4226 samples.\n","Processed 1700/4226 samples.\n","Processed 1800/4226 samples.\n","Processed 1900/4226 samples.\n","Processed 2000/4226 samples.\n","Processed 2100/4226 samples.\n","Processed 2200/4226 samples.\n","Processed 2300/4226 samples.\n","Processed 2400/4226 samples.\n","Processed 2500/4226 samples.\n","Processed 2600/4226 samples.\n","Processed 2700/4226 samples.\n","Processed 2800/4226 samples.\n","Processed 2900/4226 samples.\n","Processed 3000/4226 samples.\n","Processed 3100/4226 samples.\n","Processed 3200/4226 samples.\n","Processed 3300/4226 samples.\n","Processed 3400/4226 samples.\n","Processed 3500/4226 samples.\n","Processed 3600/4226 samples.\n","Processed 3700/4226 samples.\n","Processed 3800/4226 samples.\n","Processed 3900/4226 samples.\n","Processed 4000/4226 samples.\n","Processed 4100/4226 samples.\n","Processed 4200/4226 samples.\n"]}]},{"cell_type":"code","source":["# --- Final Display of All Experiments ---\n","comparison_df_detailed = pd.DataFrame(comparison_results_detailed)\n","comparison_df_detailed = comparison_df_detailed[['Experiment', 'R_Rank', 'Alpha', 'Epochs', 'LR', 'eval_accuracy', 'eval_precision', 'eval_recall', 'eval_f1']]\n","comparison_df_detailed.columns = ['Experiment', 'Rank (r)', 'Alpha (Î±)', 'Epochs', 'LR', 'Accuracy', 'Precision (Sarcastic)', 'Recall (Sarcastic)', 'F1-Score (Sarcastic)']\n","\n","print(\"\\n\\n--- FINAL COMPARISON OF ALL EXPERIMENTS ---\")\n","print(comparison_df_detailed.to_markdown(index=False, floatfmt=(\".0f\", \".0f\", \".0f\", \".0f\", \".1e\", \".4f\", \".4f\", \".4f\", \".4f\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z3kM6JXUec1d","outputId":"94717b03-6a9e-4665-fd95-c7e0bd8c8387","executionInfo":{"status":"ok","timestamp":1765567874744,"user_tz":300,"elapsed":14,"user":{"displayName":"Xiaozhou Wen","userId":"12001499559367279758"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--- FINAL COMPARISON OF ALL EXPERIMENTS ---\n","| Experiment                               |   Rank (r) |   Alpha (Î±) |   Epochs |      LR |   Accuracy |   Precision (Sarcastic) |   Recall (Sarcastic) |   F1-Score (Sarcastic) |\n","|:-----------------------------------------|-----------:|------------:|---------:|--------:|-----------:|------------------------:|---------------------:|-----------------------:|\n","| Llama 3.1 8B base line (LR=2e-4 1 Epoch) |         16 |          32 |        1 | 2.0e-04 |     0.9690 |                  0.9680 |               0.9846 |                 0.9762 |\n","| Llama 3.1 8B (Low Rank=8)                |          8 |          16 |        1 | 2.0e-04 |     0.9655 |                  0.9638 |               0.9835 |                 0.9736 |\n","| Llama 3.1 8B (2 Epochs)                  |         16 |          32 |        2 | 2.0e-04 |     0.9763 |                  0.9748 |               0.9890 |                 0.9818 |\n","| Llama 3.1 8B (LR=2e-5)                   |         16 |          32 |        1 | 2.0e-05 |     0.9451 |                  0.9493 |               0.9667 |                 0.9579 |\n","| Llama 3.1 8B (LR=2e-4 3 Epochs)          |         32 |          64 |        3 | 2.0e-04 |     0.9844 |                  0.9823 |               0.9938 |                 0.9880 |\n"]}]},{"cell_type":"code","source":["# Optional. This is to load the previous saved checkpoint to for inference\n","#\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# The path where you saved the merged model\n","output_dir = project_path+\"llama_3.1_8b_(low_rank=8)_2025-12-12_14-35-13\"\n","\n","# load the model from saved checkpoint\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = output_dir,\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")\n","\n","# Prepare the loaded model for faster inference\n","FastLanguageModel.for_inference(model)\n","\n","# Evaluate with test data set\n","test_metrics = evaluate_llama_model(model, tokenizer, test_dataset, EVAL_PROMPT_TEMPLATE)"],"metadata":{"id":"Q9tRmtj_qh-w"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0fcc1d4694094a2b80f650d94bcef81a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ab97416c3474dcda58f2d73f017f543","IPY_MODEL_69234508f0554d389b1ba5846fbcb6b7","IPY_MODEL_b70ab5fec6b14ee08ca582bc38190287"],"layout":"IPY_MODEL_af972cb2e5de4c4ea9a491e9aa1fd6ab"}},"0ab97416c3474dcda58f2d73f017f543":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16201be0686c48b0ab4e58b8cb01f2b0","placeholder":"â€‹","style":"IPY_MODEL_447c62bbdca64d13b2f46537512012a8","value":"model.safetensors:â€‡100%"}},"69234508f0554d389b1ba5846fbcb6b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65d8e244f4e4462e81683755bddcfb3d","max":5964186429,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e7f2babef314c1f905870774f26a465","value":5964186429}},"b70ab5fec6b14ee08ca582bc38190287":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce0b1a4e70564166a1977202999a33e8","placeholder":"â€‹","style":"IPY_MODEL_c291b5780e224914854852a821118653","value":"â€‡5.96G/5.96Gâ€‡[00:27&lt;00:00,â€‡55.8MB/s]"}},"af972cb2e5de4c4ea9a491e9aa1fd6ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16201be0686c48b0ab4e58b8cb01f2b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447c62bbdca64d13b2f46537512012a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65d8e244f4e4462e81683755bddcfb3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e7f2babef314c1f905870774f26a465":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce0b1a4e70564166a1977202999a33e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c291b5780e224914854852a821118653":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8219972a427f414db3a525e62df1113e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b390e127d7143159b53713501c5d7c5","IPY_MODEL_a1b6abc372f548228cd49da0f11968e0","IPY_MODEL_c52c0f96f6ac449fb36694249b670463"],"layout":"IPY_MODEL_5807a4dbfe6b45a4bf32f80e6370249e"}},"2b390e127d7143159b53713501c5d7c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22badf11ca9a48d9a704b30904aadf70","placeholder":"â€‹","style":"IPY_MODEL_344e3ad65efc4efeb97712d04333ba46","value":"generation_config.json:â€‡100%"}},"a1b6abc372f548228cd49da0f11968e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26b1d6aab7fc44bb8b15e8bdd24ada21","max":235,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02198143acbd4bfcaa0d6d6854493aa7","value":235}},"c52c0f96f6ac449fb36694249b670463":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97ca767794e5495ea84966e9619010a4","placeholder":"â€‹","style":"IPY_MODEL_3c8a86f84fc44f89bb30a09784c93c2c","value":"â€‡235/235â€‡[00:00&lt;00:00,â€‡28.0kB/s]"}},"5807a4dbfe6b45a4bf32f80e6370249e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22badf11ca9a48d9a704b30904aadf70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"344e3ad65efc4efeb97712d04333ba46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26b1d6aab7fc44bb8b15e8bdd24ada21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02198143acbd4bfcaa0d6d6854493aa7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"97ca767794e5495ea84966e9619010a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c8a86f84fc44f89bb30a09784c93c2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37c17d9b5a9b4e10940f27c4c006390c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c086f84300c4ea8a61bbda587c5f6dc","IPY_MODEL_daab1fb81a3d4cdea39513dc3997b589","IPY_MODEL_4755ceea85a04104826ae300c0c9871b"],"layout":"IPY_MODEL_ceaf3862458e4264856d19693980af43"}},"3c086f84300c4ea8a61bbda587c5f6dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f81933b0ddb4b469643dda26ac9f933","placeholder":"â€‹","style":"IPY_MODEL_6db7c0d6c2204ff4b83c25d80e2dd878","value":"tokenizer_config.json:â€‡"}},"daab1fb81a3d4cdea39513dc3997b589":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d97181065a14176bdc9cb28c12395a0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_416075c8b2744124b7c9d5694dbaf940","value":1}},"4755ceea85a04104826ae300c0c9871b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f35280da2ff4f79a17b8add3d4e89db","placeholder":"â€‹","style":"IPY_MODEL_7084e88775cf4bb285e759e3189aa790","value":"â€‡50.6k/?â€‡[00:00&lt;00:00,â€‡6.17MB/s]"}},"ceaf3862458e4264856d19693980af43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f81933b0ddb4b469643dda26ac9f933":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db7c0d6c2204ff4b83c25d80e2dd878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d97181065a14176bdc9cb28c12395a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"416075c8b2744124b7c9d5694dbaf940":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f35280da2ff4f79a17b8add3d4e89db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7084e88775cf4bb285e759e3189aa790":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee7b2b159d034b82970d0abbdbecaa38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d82da7c880a94220820a0c627f2e77db","IPY_MODEL_e082fc1edb12439492278477951ede08","IPY_MODEL_2511276eb190488cab7078b084e127ee"],"layout":"IPY_MODEL_3461eeab4fa543f9a5e6e7ce49822bfa"}},"d82da7c880a94220820a0c627f2e77db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ce7ab908a834b50aaa055a1fbe487cf","placeholder":"â€‹","style":"IPY_MODEL_e1547be1105f46a9920dadfc2fd369a6","value":"special_tokens_map.json:â€‡100%"}},"e082fc1edb12439492278477951ede08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b492b6c5b1db4c69801b1ac14cd9bda8","max":459,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e3b34b508e14b308bdfe53f83c2ba9f","value":459}},"2511276eb190488cab7078b084e127ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab5685148609490d8e81375d6b43f9c3","placeholder":"â€‹","style":"IPY_MODEL_56754486a89b4ebc99202562915e5456","value":"â€‡459/459â€‡[00:00&lt;00:00,â€‡62.0kB/s]"}},"3461eeab4fa543f9a5e6e7ce49822bfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ce7ab908a834b50aaa055a1fbe487cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1547be1105f46a9920dadfc2fd369a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b492b6c5b1db4c69801b1ac14cd9bda8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e3b34b508e14b308bdfe53f83c2ba9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab5685148609490d8e81375d6b43f9c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56754486a89b4ebc99202562915e5456":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3b89cf83fe14599b04cac1bd4c6ca71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f64c2cc631944e69afb9e2121391018e","IPY_MODEL_073a88a8c63f4fafa7ea671adea8cd67","IPY_MODEL_e99237f8284d4ad4a353355be5aedd7c"],"layout":"IPY_MODEL_45b2b87da21f49f8a6fcaedd95787c65"}},"f64c2cc631944e69afb9e2121391018e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8df54fe0cd974870a7d81906d916c389","placeholder":"â€‹","style":"IPY_MODEL_28d4189f389442b685f68b57e68947d3","value":"tokenizer.json:â€‡100%"}},"073a88a8c63f4fafa7ea671adea8cd67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b469bb7d656c46948e7402d8e294aaa2","max":17209920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6695d5ec9814f5a9f469fdd9de06ff4","value":17209920}},"e99237f8284d4ad4a353355be5aedd7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_766e4454a9bd4d269371a6161dd99f03","placeholder":"â€‹","style":"IPY_MODEL_d5322ca0905b40a69be650921b85da99","value":"â€‡17.2M/17.2Mâ€‡[00:01&lt;00:00,â€‡11.0MB/s]"}},"45b2b87da21f49f8a6fcaedd95787c65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8df54fe0cd974870a7d81906d916c389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28d4189f389442b685f68b57e68947d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b469bb7d656c46948e7402d8e294aaa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6695d5ec9814f5a9f469fdd9de06ff4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"766e4454a9bd4d269371a6161dd99f03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5322ca0905b40a69be650921b85da99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0bd70c1c96a41f0aa23a7ecd7106e61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3712046fcdbd4efda16897b1f11e13f6","IPY_MODEL_1fa2e7ac8c5346088b7f0e379dadc878","IPY_MODEL_edd5278563eb466095d2573cee3a4f9d"],"layout":"IPY_MODEL_cc6de16f13a94d6888bae95057b5074c"}},"3712046fcdbd4efda16897b1f11e13f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8814ecdbfdea419e981bf5b09929c0d3","placeholder":"â€‹","style":"IPY_MODEL_906c2f5d951f40eea886ad577b63115b","value":"Map:â€‡100%"}},"1fa2e7ac8c5346088b7f0e379dadc878":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd59820a83f84fdea610f8db1f0ec6d3","max":33802,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a8e23e7654f44f8acfcc1a38241df97","value":33802}},"edd5278563eb466095d2573cee3a4f9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaab130ba18b4f66b31bb5cfb145d6f3","placeholder":"â€‹","style":"IPY_MODEL_525019f0e04d4c47bdf3d9eb17394bba","value":"â€‡33802/33802â€‡[00:00&lt;00:00,â€‡94931.70â€‡examples/s]"}},"cc6de16f13a94d6888bae95057b5074c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8814ecdbfdea419e981bf5b09929c0d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"906c2f5d951f40eea886ad577b63115b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd59820a83f84fdea610f8db1f0ec6d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a8e23e7654f44f8acfcc1a38241df97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaab130ba18b4f66b31bb5cfb145d6f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"525019f0e04d4c47bdf3d9eb17394bba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"638d44abb345436bb67568d088c460ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16450f1a163841958dd621f5cd0c9054","IPY_MODEL_62c8b0452132405bbc070f502fc88a22","IPY_MODEL_18cea43d35024f189741f19cbb880da2"],"layout":"IPY_MODEL_f3d4effcd0674462a83dc49ef36296e1"}},"16450f1a163841958dd621f5cd0c9054":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5745381ab7a42eb884b2a162e4f0f60","placeholder":"â€‹","style":"IPY_MODEL_a29f5df9e3274ec58d152b0de2cf555a","value":"Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=16):â€‡100%"}},"62c8b0452132405bbc070f502fc88a22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61930b55964e4efd8a79d72ce3028c60","max":33802,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0ae3d85951f4a03b006adbf52825ecd","value":33802}},"18cea43d35024f189741f19cbb880da2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87299a8c35a443fd99c013494fcfcc35","placeholder":"â€‹","style":"IPY_MODEL_6bdb0095e1c3498da1c7185560f2fc1b","value":"â€‡33802/33802â€‡[00:06&lt;00:00,â€‡8765.99â€‡examples/s]"}},"f3d4effcd0674462a83dc49ef36296e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5745381ab7a42eb884b2a162e4f0f60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a29f5df9e3274ec58d152b0de2cf555a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61930b55964e4efd8a79d72ce3028c60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ae3d85951f4a03b006adbf52825ecd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87299a8c35a443fd99c013494fcfcc35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bdb0095e1c3498da1c7185560f2fc1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da8b6f630e3c4723ac2adddb52d75df0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ecae2efff8b344c9a5740b7d1c206539","IPY_MODEL_66624584d4924bff883718baa68a254d","IPY_MODEL_3b043ba68e544d518c2a9215f893da55"],"layout":"IPY_MODEL_a1ed36318657498b851300d045e14955"}},"ecae2efff8b344c9a5740b7d1c206539":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e16a799cf404f0caf44c67d6647c36a","placeholder":"â€‹","style":"IPY_MODEL_cd014f82044a44e88788dd8103d70114","value":"Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=16):â€‡100%"}},"66624584d4924bff883718baa68a254d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aaf99aeb6ba46e0a6aba052273c4813","max":33802,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19d0359a97c24367b0f0fc19749e8cba","value":33802}},"3b043ba68e544d518c2a9215f893da55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea45e738b41479fb1c3a44e868c35b5","placeholder":"â€‹","style":"IPY_MODEL_c431a0afdfc449daabb438fd5d1c0bf7","value":"â€‡33802/33802â€‡[00:07&lt;00:00,â€‡8398.69â€‡examples/s]"}},"a1ed36318657498b851300d045e14955":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e16a799cf404f0caf44c67d6647c36a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd014f82044a44e88788dd8103d70114":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aaf99aeb6ba46e0a6aba052273c4813":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19d0359a97c24367b0f0fc19749e8cba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dea45e738b41479fb1c3a44e868c35b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c431a0afdfc449daabb438fd5d1c0bf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57900396aad44394b325dac3e140bd8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db7ceefd371049709fcc719140c4c713","IPY_MODEL_98b29cc31ab9416aaa74c466efef9918","IPY_MODEL_ccd44101c1914bb0a4f18e32448a60f6"],"layout":"IPY_MODEL_14941fe804a748fbada2021ff6068fb0"}},"db7ceefd371049709fcc719140c4c713":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d8a7c532b9143128d21a0dcee4de073","placeholder":"â€‹","style":"IPY_MODEL_a8e634f43282449ab54ec7bdb5947eba","value":"Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=16):â€‡100%"}},"98b29cc31ab9416aaa74c466efef9918":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5138df876c2b473696008b72df478bc8","max":33802,"min":0,"orientation":"horizontal","style":"IPY_MODEL_270a5373b0b44103a33a1bbdd158f085","value":33802}},"ccd44101c1914bb0a4f18e32448a60f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84a592ce933e470a8dbe0083c627d404","placeholder":"â€‹","style":"IPY_MODEL_bd7a8cccb3b94df8803b2ff867d8e059","value":"â€‡33802/33802â€‡[00:07&lt;00:00,â€‡7018.52â€‡examples/s]"}},"14941fe804a748fbada2021ff6068fb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d8a7c532b9143128d21a0dcee4de073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8e634f43282449ab54ec7bdb5947eba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5138df876c2b473696008b72df478bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"270a5373b0b44103a33a1bbdd158f085":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84a592ce933e470a8dbe0083c627d404":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7a8cccb3b94df8803b2ff867d8e059":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3b7dc79349d4632a23a7c1b04437f66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af2998a2333a430abd42236a527bd21f","IPY_MODEL_9d32a2de7b464bf6bba267c047bcd667","IPY_MODEL_0c0558a744d949e58c6da6b79e515920"],"layout":"IPY_MODEL_3a8fc6c603064f35a625ebcd921b91f9"}},"af2998a2333a430abd42236a527bd21f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46430e98516e4cda97b6378ef654a543","placeholder":"â€‹","style":"IPY_MODEL_585d0e338e35407689e0fa5aacbc7171","value":"Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=16):â€‡100%"}},"9d32a2de7b464bf6bba267c047bcd667":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df54201603234e65895a03fc55478bb8","max":33802,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88e25116699c411fbde04b012d7e5a88","value":33802}},"0c0558a744d949e58c6da6b79e515920":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a4804a5e0044776a013463733f0fe9e","placeholder":"â€‹","style":"IPY_MODEL_b0ea38b9ae4f4baa8653507231bf000e","value":"â€‡33802/33802â€‡[00:07&lt;00:00,â€‡8398.46â€‡examples/s]"}},"3a8fc6c603064f35a625ebcd921b91f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46430e98516e4cda97b6378ef654a543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"585d0e338e35407689e0fa5aacbc7171":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df54201603234e65895a03fc55478bb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88e25116699c411fbde04b012d7e5a88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a4804a5e0044776a013463733f0fe9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0ea38b9ae4f4baa8653507231bf000e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed1b5a96cb3a44e3990f569a70eeb5bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fdd087aeb61435d9572f80ac5d3356b","IPY_MODEL_1778d7dbf638447ba642fad1df9830fe","IPY_MODEL_e9722a2803af468e99ac0f26782f6767"],"layout":"IPY_MODEL_f7e21e7e3d3944baaa585e203f29ad1f"}},"0fdd087aeb61435d9572f80ac5d3356b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc2d6ec07a7f4b86b34069ed0dc16e71","placeholder":"â€‹","style":"IPY_MODEL_793fa850d1de4525a6708cf7bc9586b0","value":"Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=16):â€‡100%"}},"1778d7dbf638447ba642fad1df9830fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1622de9d47d747eab0198e7137e2891a","max":33802,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aea0467b05774311ba17f518454e8873","value":33802}},"e9722a2803af468e99ac0f26782f6767":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a5f5a1a744e4fcb9efc8865d2ef24c4","placeholder":"â€‹","style":"IPY_MODEL_6f31d41554fb4fc08dd88a322302400b","value":"â€‡33802/33802â€‡[00:07&lt;00:00,â€‡8970.96â€‡examples/s]"}},"f7e21e7e3d3944baaa585e203f29ad1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc2d6ec07a7f4b86b34069ed0dc16e71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"793fa850d1de4525a6708cf7bc9586b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1622de9d47d747eab0198e7137e2891a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aea0467b05774311ba17f518454e8873":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a5f5a1a744e4fcb9efc8865d2ef24c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f31d41554fb4fc08dd88a322302400b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}